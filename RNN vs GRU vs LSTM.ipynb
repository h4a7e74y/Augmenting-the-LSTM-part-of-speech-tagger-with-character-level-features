{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "* https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19015280af0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.5525,  0.6355, -0.3968]]), tensor([[-0.6571, -1.6428,  0.9803]]), tensor([[-0.0421, -0.8206,  0.3133]]), tensor([[-1.1352,  0.3773, -0.2824]]), tensor([[-2.5667, -1.4303,  0.5009]])]\n",
      "\n",
      "(tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n",
      "\n",
      "tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "\n",
    "# input_size – The number of expected features in the input x\n",
    "# hidden_size – The number of features in the hidden state h\n",
    "# num_layers – Number of recurrent layers. E.g., setting num_layers=2 \n",
    "#              would mean stacking two LSTMs together to form a stacked LSTM, \n",
    "#              with the second LSTM taking in outputs of the first LSTM and \n",
    "#              computing the final results. Default: 1\n",
    "# bias – If False, then the layer does not use bias weights b_ih and b_hh. \n",
    "#              Default: True\n",
    "# batch_first – If True, then the input and output tensors are provided as \n",
    "#               (batch, seq, feature). Default: False\n",
    "# dropout – If non-zero, introduces a Dropout layer on the outputs of each \n",
    "#               LSTM layer except the last layer, with dropout probability \n",
    "#               equal to dropout. Default: 0\n",
    "# bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
    "\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "print(inputs)\n",
    "print()\n",
    "print(hidden)\n",
    "print()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5525,  0.6355, -0.3968]],\n",
      "\n",
      "        [[-0.6571, -1.6428,  0.9803]],\n",
      "\n",
      "        [[-0.0421, -0.8206,  0.3133]],\n",
      "\n",
      "        [[-1.1352,  0.3773, -0.2824]],\n",
      "\n",
      "        [[-2.5667, -1.4303,  0.5009]]])\n",
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "print(inputs)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is as follows: let our input sentence be w1,…,wM, where wi∈V, our vocab. Also, let T be our tag set, and yi the tag of word wi. Denote our prediction of the tag of word wi by y^i.\n",
    "\n",
    "This is a structure prediction, model, where our output is a sequence y^1,…,y^M, where y^i∈T.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden state at timestep i as hi. Also, assign each tag a unique index (like how we had word_to_ix in the word embeddings section). Then our prediction rule for y^i is\n",
    "\n",
    "y^i=argmaxj (logSoftmax(Ahi+b))j\n",
    "That is, take the log softmax of the affine map of the hidden state, and the predicted tag is the tag that has the maximum value in this vector. Note this implies immediately that the dimensionality of the target space of A is |T|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "{' ': 0, 'T': 1, 'h': 2, 'e': 3, 'd': 4, 'o': 5, 'g': 6, 'a': 7, 't': 8, 'p': 9, 'l': 10, 'E': 11, 'v': 12, 'r': 13, 'y': 14, 'b': 15, 'k': 16}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "#парсинг по буквенно            \n",
    "char_to_ix = {}\n",
    "char_to_ix[' '] = len(char_to_ix)\n",
    "for sent, _ in training_data:\n",
    "    for word in sent:\n",
    "        for char in word:\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "                \n",
    "\n",
    "print(word_to_ix)\n",
    "print(char_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "WORD_EMBEDDING_DIM = 6\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "HIDDEN_DIM = 6\n",
    "MAX_WORD_LEN = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, hidden_dim, vocab_size, tagset_size, char_size):\n",
    "        #добавил char_embedding_dim, char_size\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        #добавил char_embedding_dim в инициализацию\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        #добавил char_embedding_dim в инициализацию\n",
    "        self.char_embedding = nn.Embedding(char_size, char_embedding_dim)\n",
    "        \n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        \n",
    "        #подаю сумму word_embedding_dim и char_embedding_dim\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_embedding_dim, hidden_dim)\n",
    "        #подаю два char_embedding_dim в сеть\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_embedding_dim)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sent_word, sent_char, max_word_len):\n",
    "        # char\n",
    "        sent_size = sent_word.size()[0]\n",
    "        char_emb = self.char_embedding(sent_char)\n",
    "        try :\n",
    "            char_emb = char_emb.view(len(sent_word), max_word_len, -1).permute(1,0,2)\n",
    "        except :\n",
    "            print(\"Char embedding size:\",char_emb.size())\n",
    "\n",
    "        self.hidden_char = self.initHidden_char(sent_size)\n",
    "        char_lstm_out, self.hidden = self.char_lstm(char_emb, self.hidden_char)\n",
    "        char_embeded = char_lstm_out[-1,:,:].view(sent_size,-1)\n",
    "        \n",
    "        # word emb\n",
    "        word_embeded = self.word_embedding(sent_word)\n",
    "\n",
    "        embeds = torch.cat((word_embeded, char_embeded),dim=1)\n",
    "        # print('embeded size:\\n', embeded.size())\n",
    "        self.hidden = self.initHidden()\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(sent_size,1,-1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(sent_size,-1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = (Variable(torch.zeros(1,1,self.hidden_dim)),\n",
    "                  Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "        return result\n",
    "\n",
    "    def initHidden_char(self, sent_size):\n",
    "        result = (Variable(torch.zeros(1, sent_size, self.char_embedding_dim)),\n",
    "                  Variable(torch.zeros(1, sent_size, self.char_embedding_dim)))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores tensor([[-0.8637, -1.1508, -1.3394],\n",
      "        [-0.8478, -1.1860, -1.3234],\n",
      "        [-0.9153, -1.0591, -1.3751],\n",
      "        [-0.9468, -1.0077, -1.3985],\n",
      "        [-0.9017, -1.0278, -1.4424]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.7230, -0.0105, -6.4361],\n",
      "        [-3.2792, -4.8884, -0.0462],\n",
      "        [-0.0470, -4.4091, -3.3877],\n",
      "        [-3.8391, -0.0232, -6.5308]], grad_fn=<LogSoftmaxBackward>)\n",
      "after training\n",
      "tensor([[-0.0414, -3.8744, -3.9238],\n",
      "        [-3.5818, -0.0300, -6.3782],\n",
      "        [-3.0147, -5.0246, -0.0572],\n",
      "        [-0.0398, -4.1710, -3.7487],\n",
      "        [-3.8825, -0.0245, -5.6284]])\n",
      "targets:\n",
      " tensor([0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(WORD_EMBEDDING_DIM,CHAR_EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix), len(char_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    print('before training')\n",
    "    word_inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    sent_chars = []\n",
    "    for w in training_data[0][0]:\n",
    "        space_sign = ' ' * (MAX_WORD_LEN - len(w))\n",
    "        sent_chars.extend(list(space_sign + w) if len(w) < MAX_WORD_LEN else list(w[:MAX_WORD_LEN]))\n",
    "    char_inputs = prepare_sequence(sent_chars, char_to_ix)\n",
    "\n",
    "    tag_scores = model(word_inputs, char_inputs, MAX_WORD_LEN)\n",
    "    targets = prepare_sequence(training_data[0][1], tag_to_ix)\n",
    "    print(targets)\n",
    "    print(\"tag_scores\",tag_scores)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #инициализация скрытого слоя\n",
    "        model.hidden = model.initHidden()\n",
    "        \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        word_inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        sent_chars = []\n",
    "        for w in sentence:\n",
    "            space_sign = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sent_chars.extend(list(space_sign + w) if len(w)<MAX_WORD_LEN else list(w[:MAX_WORD_LEN]))\n",
    "        char_inputs = prepare_sequence(sent_chars, char_to_ix)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(word_inputs, char_inputs, MAX_WORD_LEN)\n",
    "\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('after training')\n",
    "    word_inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    sent_chars = []\n",
    "    for w in training_data[0][0]:\n",
    "        space_sign = ' ' * (MAX_WORD_LEN - len(w))\n",
    "        sent_chars.extend(list(space_sign + w) if len(w) < MAX_WORD_LEN else list(w[:MAX_WORD_LEN]))\n",
    "    char_inputs = prepare_sequence(sent_chars, char_to_ix)\n",
    "\n",
    "    tag_scores = model(word_inputs, char_inputs, MAX_WORD_LEN)\n",
    "    targets = prepare_sequence(training_data[0][1], tag_to_ix)\n",
    "    print(tag_scores)\n",
    "    print('targets:\\n',targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Augmenting the LSTM part-of-speech tagger with character-level features\n",
    "In the example above, each word had an embedding, which served as the inputs to our sequence model. Let’s augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English.\n",
    "\n",
    "To do this, let cw be the character-level representation of word w. Let xw be the word embedding as before. Then the input to our sequence model is the concatenation of xw and cw. So if xw has dimension 5, and cw dimension 3, then our LSTM should accept an input of dimension 8.\n",
    "\n",
    "To get the character level representation, do an LSTM over the characters of a word, and let cw be the final hidden state of this LSTM. Hints:\n",
    "\n",
    "There are going to be two LSTM’s in your new model. The original one that outputs POS tag scores, and the new one that outputs a character-level representation of each word.\n",
    "To do a sequence model over characters, you will have to embed characters. The character embeddings will be the input to the character LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'T': 1, 'h': 2, 'e': 3, 'd': 4, 'o': 5, 'g': 6, 'a': 7, 't': 8, 'p': 9, 'l': 10, 'E': 11, 'v': 12, 'r': 13, 'y': 14, 'b': 15, 'k': 16}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {}\n",
    "char_to_ix[' '] = len(char_to_ix)\n",
    "for sent, _ in training_data:\n",
    "    for word in sent:\n",
    "        for char in word:\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
